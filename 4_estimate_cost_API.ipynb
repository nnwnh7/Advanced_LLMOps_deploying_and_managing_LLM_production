{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PW25pYWTXzX"
      },
      "source": [
        "# Solution: Estimating costs of an LLM API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiTjK-U_TXzY"
      },
      "source": [
        "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LinkedInLearning/generative-ai-and-llmops-deploying-and-managing-llms-in-production-4465782/blob/solution/ch-06/challenge_estimating_API_costs.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLopcvd3Cl96",
        "outputId": "7a0726ba-bbf8-47de-d66e-77618c3e51d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install groc openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2_289xWjClMq"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import getpass\n",
        "import os\n",
        "from groq import Groq\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxmQQF9ICpQu",
        "outputId": "e67ae4f8-aa37-4d38-88b6-6a7ab1c288b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "client = Groq(api_key=getpass.getpass())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    total_cost=(input_tokens * input_token_cost) + (output_tokens * output_token_cost)\n",
        "    return total_cost\n",
        "\n",
        "def generate_text_and_calculate_cost(prompt, model=\"llama3-8b-8192\"):\n",
        "    response = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "            )\n",
        "\n",
        "    generated_output = response.choices[0].message.content\n",
        "\n",
        "    # Get the number of tokens used\n",
        "    input_tokens = response.usage.prompt_tokens\n",
        "    output_tokens = response.usage.completion_tokens\n",
        "    cost = calculate_cost(input_tokens, output_tokens)\n",
        "\n",
        "    return generated_output, input_tokens, output_tokens, cost\n",
        "\n",
        "\n",
        "prompt = \"Write a blog about taking Generative AI applications to production.\"\n",
        "generated_output, input_tokens, output_tokens, cost = generate_text_and_calculate_cost(prompt)\n",
        "\n",
        "print(f\"Total input tokens: {input_tokens}\")\n",
        "print(f\"Total output tokens: {output_tokens}\")\n",
        "print(f\"Cost of the API call: ${cost:.4f}\")"
      ],
      "metadata": {
        "id": "4nYtrWa2T45g",
        "outputId": "5395a520-e4d6-4225-8e3b-92c476f82457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total input tokens: 22\n",
            "Total output tokens: 807\n",
            "Cost of the API call: $0.0244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cGrZ63Ll1txH"
      },
      "outputs": [],
      "source": [
        "input_token_cost = 0.00001\n",
        "output_token_cost = 0.00003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SRYZUQ9_uTEo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5Sd7dUJDqi0"
      },
      "source": [
        "## Using tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgSIDplFDqST",
        "outputId": "e1cd383a-d414-40ff-9299-1909e8a16a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOoa-r4CDtTE",
        "outputId": "9450e559-2c39-4680-e501-c0b1af149a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost of the prompt: $0.0244\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def calculate_cost(input_prompt, generated_output, model):\n",
        "    # Initialize the tiktoken encoding for the model\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "\n",
        "    # Encode the prompt to get the number of tokens\n",
        "    input_tokens = len(encoding.encode(input_prompt))\n",
        "    output_tokens = len(encoding.encode(generated_output))\n",
        "\n",
        "    # Calculate the cost\n",
        "    total_cost=(input_tokens * input_token_cost) + (output_tokens * output_token_cost)\n",
        "\n",
        "    return total_cost\n",
        "\n",
        "model=\"gpt-4\"\n",
        "# model=\"gpt-3.5-turbo\"\n",
        "\n",
        "total_cost = calculate_cost(prompt, generated_output, model)\n",
        "print(f\"Cost of the prompt: ${cost:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a tokenizer that matches Llama 3’s tokenization style\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")  # Works with many models\n",
        "\n",
        "text = \"New advancements in self-healing materials are promising.\"\n",
        "tokens = encoding.encode(text)\n",
        "\n",
        "print(\"Tokenized Output:\", tokens)\n",
        "print(\"Token Count:\", len(tokens))"
      ],
      "metadata": {
        "id": "Delan6OXXNde",
        "outputId": "1336a4d6-9981-449d-bbb2-cc15709972f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Output: [3648, 83787, 304, 659, 38435, 6260, 7384, 527, 26455, 13]\n",
            "Token Count: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haexG56E3U-h"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}